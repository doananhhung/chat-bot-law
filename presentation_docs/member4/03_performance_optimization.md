# âš¡ Performance Optimization

## Má»¥c tiÃªu há»c táº­p
Sau khi Ä‘á»c tÃ i liá»‡u nÃ y, báº¡n sáº½ hiá»ƒu:
- CÃ¡c bottlenecks trong há»‡ thá»‘ng
- Chiáº¿n lÆ°á»£c caching vá»›i Streamlit
- Query optimization techniques

---

## 1. Performance Challenges

### 1.1 Initial State (No Optimization)

```
Cold Start Problem:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User opens app                                               â”‚
â”‚     â””â”€â”€ Load Embedding Model (~17s) â† SLOW!                 â”‚
â”‚         â””â”€â”€ Load FAISS Index (~0.5s)                        â”‚
â”‚             â””â”€â”€ Initialize LLMs (~1s)                       â”‚
â”‚                 â””â”€â”€ Ready to chat (~18.5s total)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Every Page Reload:
Same 18.5s delay! ğŸ˜¢
```

### 1.2 Target State (Optimized)

```
First Load: ~17s (unavoidable - model weight download)
Subsequent Loads: < 1s âœ…
Query Response: ~1-2s âœ…
```

---

## 2. Caching Strategy

### 2.1 @st.cache_resource

**Purpose**: Cache objects across all sessions and reruns

```python
@st.cache_resource(show_spinner="Äang khá»Ÿi Ä‘á»™ng Model & Index...")
def get_retriever():
    """Load ONCE, reuse forever."""
    return SemanticRetriever()

@st.cache_resource(show_spinner="Äang káº¿t ná»‘i AI...")
def get_rag_chain():
    """Load ONCE, reuse across all users."""
    retriever = get_retriever()
    return RAGChain(retriever)
```

### 2.2 Before vs After

| Metric | Before | After |
|--------|--------|-------|
| **First load** | 17s | 17s |
| **Page reload** | 17s | <1s âœ… |
| **New session** | 17s | <1s âœ… |

### 2.3 What Gets Cached

```
Cached (Singleton):
â”œâ”€â”€ Embedding Model (~1.5GB)
â”œâ”€â”€ FAISS Index (~10MB)
â”œâ”€â”€ LLM connections (3 instances)
â””â”€â”€ RAGChain orchestrator

NOT Cached (Per-session):
â”œâ”€â”€ st.session_state
â”œâ”€â”€ Database connections
â””â”€â”€ Chat messages
```

---

## 3. Stateless RAGChain Design

### 3.1 Problem: Stateful RAG

```python
# BAD: Stateful design
class RAGChain:
    def __init__(self):
        self.history = []  # â† State stored here
    
    def answer(self, query):
        # Uses internal history
        pass

# Cannot cache! Each user needs different history.
```

### 3.2 Solution: Stateless RAG

```python
# GOOD: Stateless design
class RAGChain:
    def __init__(self):
        # No internal history!
        pass
    
    def answer(self, query, history_str):  # â† History injected
        # Uses externally provided history
        pass

# Can cache! History managed externally by app.py
```

### 3.3 Implementation

```python
# app.py
def format_chat_history(messages):
    """Convert DB messages to string for LLM."""
    buffer = ""
    for msg in messages:
        role = "User" if msg.role == "user" else "AI"
        buffer += f"{role}: {msg.content}\n"
    return buffer

# Usage
history_str = format_chat_history(db_messages)
response = rag_chain.generate_answer(query, chat_history_str=history_str)
```

---

## 4. Database Connection Management

### 4.1 Problem: Connection Leaks

```python
# BAD: Connection never closed
db = SessionLocal()
repo = ChatRepository(db)
# ... use repo ...
# Forgot to close!

# Result: Connection pool exhausted after many reruns
```

### 4.2 Solution: try/finally Pattern

```python
# GOOD: Always close
db = get_db_session()
try:
    repo = ChatRepository(db)
    # ... all logic here ...
finally:
    db.close()  # Always executes, even on error
```

### 4.3 Effect

| Before | After |
|--------|-------|
| Memory grows | Stable memory |
| DB locks | No locks |
| Crashes after ~100 reruns | Runs indefinitely |

---

## 5. FAISS Index Optimization

### 5.1 Index Types Performance

| Index | Latency (1500 vec) | Latency (100K vec) | Accuracy |
|-------|-------------------|-------------------|----------|
| Flat | 138ms | 5000ms | 100% |
| IVF nprobe=8 | 87ms | 200ms | 73% |
| IVF nprobe=32 | 94ms | 300ms | 97% |
| IVF nprobe=64 | 93ms | 400ms | 100% |

### 5.2 Recommended Configuration

```bash
# For current dataset (1500 vectors)
VECTOR_INDEX_TYPE=ivf
IVF_NLIST=64
IVF_NPROBE=32  # 97% recall, minimal latency impact
```

### 5.3 Runtime Mode Switching

```python
# Allow users to trade accuracy for speed
retriever.set_search_mode("speed")    # nprobe=2
retriever.set_search_mode("balanced") # nprobe=8
retriever.set_search_mode("quality")  # nprobe=64
```

---

## 6. Query Latency Breakdown

### 6.1 Typical Query Flow

```
User Query: "Thai sáº£n Ä‘Æ°á»£c nghá»‰ máº¥y thÃ¡ng?"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step                              â”‚ Time               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Query Rewriting (LLM call)     â”‚ ~200-300ms        â”‚
â”‚ 2. Intent Classification (LLM)    â”‚ ~200-300ms        â”‚
â”‚ 3. Query Embedding                â”‚ ~80-100ms         â”‚
â”‚ 4. FAISS Search                   â”‚ ~10-30ms          â”‚
â”‚ 5. Context Formatting             â”‚ ~5ms              â”‚
â”‚ 6. Answer Generation (LLM)        â”‚ ~500-1000ms       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL                             â”‚ ~1-2 seconds      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Bottleneck Analysis

```
LLM calls: ~80% of latency
         â†ª 3 calls Ã— ~300-500ms each
         
Local processing: ~20%
         â†ª Embedding: ~100ms
         â†ª FAISS: ~20ms
         â†ª Others: ~50ms
```

---

## 7. LLM Optimization

### 7.1 Parallel LLM Selection

```bash
# Use lightweight model for Router/Rewriter
ROUTER_MODEL_NAME=smaller_model    # If available
REWRITER_MODEL_NAME=smaller_model

# Use powerful model for Generator only
LLM_MODEL_NAME=kimi-k2-instruct-0905
```

### 7.2 Groq vs Google

| Provider | Avg Response | Max Tokens |
|----------|-------------|------------|
| **Groq** | ~300-500ms | 8K |
| Google Gemini | ~500-1000ms | 32K |

**Recommendation**: Use Groq for speed âœ…

---

## 8. Cache Invalidation

### 8.1 When to Clear Cache

```python
# After index update
if st.button("ğŸ”„ Cáº­p nháº­t Index"):
    VectorIndexer.sync_index()
    st.cache_resource.clear()  # Clear all cached resources
    st.rerun()
```

### 8.2 What Happens

```
st.cache_resource.clear()
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Clear:                                  â”‚
â”‚ - get_retriever() â†’ new SemanticRetrieverâ”‚
â”‚ - get_rag_chain() â†’ new RAGChain        â”‚
â”‚                                         â”‚
â”‚ Next call will:                         â”‚
â”‚ - Reload updated FAISS index            â”‚
â”‚ - Recreate LLM connections              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. Memory Optimization

### 9.1 Current Memory Usage

| Component | Memory |
|-----------|--------|
| Embedding Model | ~1.5 GB |
| FAISS Index (1500 vec) | ~10 MB |
| LLM Connections | ~100 MB |
| App overhead | ~200 MB |
| **Total** | **~2 GB** |

### 9.2 Optimization Options (Not implemented)

```python
# Option 1: Use GPU (if available)
model_kwargs={'device': 'cuda'}  # Instead of 'cpu'

# Option 2: Quantized model
model_name = "smaller-quantized-model"

# Option 3: External embedding API
# â†’ Removes 1.5GB model from memory
# â†’ Adds network latency
```

---

## 10. Benchmark Results

### 10.1 System Specs

```
CPU: [Your CPU]
RAM: [Your RAM]
GPU: None (CPU-only)
```

### 10.2 Actual Measurements

| Metric | Value |
|--------|-------|
| Cold start | ~17s |
| Warm page load | <1s |
| Average query | 1.2-1.8s |
| Embedding latency | ~90ms |
| FAISS search | ~20ms |
| LLM generation | ~800ms |

---

## 11. Optimization Summary

| Technique | Impact | Implemented |
|-----------|--------|-------------|
| **@st.cache_resource** | Page reload: 17s â†’ <1s | âœ… |
| **Stateless RAGChain** | Enable caching | âœ… |
| **try/finally DB** | Prevent leaks | âœ… |
| **IVF Index** | Large dataset ready | âœ… |
| **Search modes** | User control | âœ… |
| **Groq LLM** | Faster inference | âœ… |

---

## 12. Key Takeaways

> [!IMPORTANT]
> **Äiá»ƒm nháº¥n khi thuyáº¿t trÃ¬nh:**
> 1. **@st.cache_resource**: Biáº¿n 17s thÃ nh <1s cho page reload
> 2. **Stateless design**: Cho phÃ©p caching RAGChain
> 3. **IVF index**: Ready cho scale lÃªn 100K vectors
> 4. **Groq API**: Fast inference cho real-time chat

---

## TÃ i liá»‡u liÃªn quan
- [Streamlit UI](./01_streamlit_ui.md)
- [Demo Script](./04_demo_script.md)

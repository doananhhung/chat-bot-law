# ðŸ“¥ Ingestion Pipeline - Luá»“ng Náº¡p Dá»¯ Liá»‡u

## Má»¥c tiÃªu há»c táº­p
Sau khi Ä‘á»c tÃ i liá»‡u nÃ y, báº¡n sáº½ hiá»ƒu:
- Tá»•ng quan pipeline ingestion
- CÃ¡c bÆ°á»›c trong quÃ¡ trÃ¬nh náº¡p dá»¯ liá»‡u
- CÆ¡ cháº¿ incremental sync

---

## 1. Tá»•ng quan Ingestion Pipeline

### 1.1 Äá»‹nh nghÄ©a
**Ingestion Pipeline** lÃ  quy trÃ¬nh chuyá»ƒn Ä‘á»•i tÃ i liá»‡u gá»‘c (PDF/DOCX) thÃ nh dáº¡ng cÃ³ thá»ƒ tÃ¬m kiáº¿m Ä‘Æ°á»£c (vectors trong FAISS).

### 1.2 CÃ¡c bÆ°á»›c chÃ­nh

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INGESTION PIPELINE                        â”‚
â”‚                                                             â”‚
â”‚   PDF/DOCX          Text            Chunks          Vectors â”‚
â”‚   Files    â”€â”€â”€â”€â”€â”€â–º  Docs  â”€â”€â”€â”€â”€â”€â–º   List  â”€â”€â”€â”€â”€â”€â–º  Index   â”‚
â”‚                                                             â”‚
â”‚            LOAD          SPLIT          EMBED       STORE   â”‚
â”‚         (loader.py)  (splitter.py)  (indexer.py)  (FAISS)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Luá»“ng xá»­ lÃ½ chi tiáº¿t

### 2.1 Step 1: LOAD - Äá»c tÃ i liá»‡u

**File**: `src/ingestion/loader.py`

```python
class DocumentLoader:
    SUPPORTED_EXTENSIONS = {
        ".pdf": PyPDFLoader,
        ".docx": Docx2txtLoader,
        ".doc": Docx2txtLoader
    }
```

**Chá»©c nÄƒng**:
- Äá»c file PDF sá»­ dá»¥ng `PyPDFLoader` (LangChain)
- Äá»c file DOCX sá»­ dá»¥ng `Docx2txtLoader`
- TrÃ­ch xuáº¥t text vÃ  metadata (tÃªn file, sá»‘ trang)

**Output**:
```python
Document(
    page_content="Äiá»u 139. Nghá»‰ thai sáº£n...",
    metadata={
        "source": "luat_lao_dong.pdf",
        "page": 0  # 0-indexed
    }
)
```

---

### 2.2 Step 2: SPLIT - Chia thÃ nh chunks

**File**: `src/ingestion/splitter.py`

**Chiáº¿n lÆ°á»£c**: `RecursiveCharacterTextSplitter`

```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Tá»‘i Ä‘a 1000 kÃ½ tá»±
    chunk_overlap=200,    # Overlap 200 kÃ½ tá»± vá»›i chunk liá»n ká»
    separators=["\n\n", "\n", " ", ""]  # Æ¯u tiÃªn cáº¯t theo paragraph
)
```

**Táº¡i sao cáº§n chunking?**
1. LLM cÃ³ giá»›i háº¡n context window
2. TÃ¬m kiáº¿m chÃ­nh xÃ¡c hÆ¡n vá»›i chunks nhá»
3. Giáº£m noise - chá»‰ láº¥y pháº§n liÃªn quan

**Visualization**:
```
Original Document (5000 chars):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Äiá»u 139... text... Äiá»u 140... text... Äiá»u 141. â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After Splitting (5 chunks):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk 1  â”‚  â”‚ Chunk 2  â”‚  â”‚ Chunk 3  â”‚  â”‚ Chunk 4  â”‚  â”‚ Chunk 5  â”‚
â”‚ ~1000    â”‚  â”‚ ~1000    â”‚  â”‚ ~1000    â”‚  â”‚ ~1000    â”‚  â”‚ ~1000    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†˜â”€â”€overlapâ”€â”€â†™        â†˜â”€â”€overlapâ”€â”€â†™
```

**Metadata sau khi split**:
```python
chunk.metadata = {
    "source": "luat_lao_dong.pdf",
    "page": 5,
    "chunk_id": "abc123",       # UUID
    "chunk_index": 0,           # Thá»© tá»± trong file
    "total_chunks": 150,        # Tá»•ng chunks cá»§a file
    "created_at": "2026-01-27T..."
}
```

---

### 2.3 Step 3: EMBED - Chuyá»ƒn text â†’ vector

**File**: `src/ingestion/indexer.py`

**Model**: `bkai-foundation-models/vietnamese-bi-encoder`

```python
embeddings = HuggingFaceEmbeddings(
    model_name="bkai-foundation-models/vietnamese-bi-encoder",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)
```

**Process**:
```
Text: "Nghá»‰ thai sáº£n Ä‘Æ°á»£c 6 thÃ¡ng"
      â”‚
      â–¼
Tokenizer â†’ [Token1, Token2, ..., TokenN]
      â”‚
      â–¼
BiEncoder Model
      â”‚
      â–¼
Vector: [0.12, -0.34, 0.56, ..., 0.78]  (768 dimensions)
```

**Batch Processing**:
```python
texts = [chunk.page_content for chunk in chunks]
embeddings_matrix = np.array(embeddings.embed_documents(texts))
# Shape: (num_chunks, 768)
```

---

### 2.4 Step 4: INDEX - LÆ°u vÃ o FAISS

**File**: `src/ingestion/indexer.py`

**Index Types**:

| Type | Factory String | Äáº·c Ä‘iá»ƒm |
|------|----------------|----------|
| Flat | `"Flat"` | Exact search, brute-force |
| IVF | `"IVF64,Flat"` | Clustering, approximate |
| IVFPQ | `"IVF64,PQ48x8"` | Clustering + compression |

**IVF Index Creation**:
```python
# 1. Create index using factory
index = faiss.index_factory(dimension, "IVF64,Flat", faiss.METRIC_L2)

# 2. Train index (K-means clustering)
if not index.is_trained:
    index.train(embeddings_matrix)  # Learn 64 cluster centroids

# 3. Add vectors
index.add(embeddings_matrix)

# 4. Save to disk
vector_store.save_local("data/vector_store/")
```

**Output Files**:
```
data/vector_store/
â”œâ”€â”€ index.faiss          # FAISS binary index
â”œâ”€â”€ index.pkl            # LangChain metadata (docstore mapping)
â””â”€â”€ indexing_metadata.json  # File tracking metadata
```

---

## 3. Incremental Sync (Differential Indexing)

### 3.1 Váº¥n Ä‘á» vá»›i Full Rebuild
- Rebuild toÃ n bá»™ khi thÃªm 1 file má»›i = lÃ£ng phÃ­
- Vá»›i 1000 files, thÃªm 1 file pháº£i xá»­ lÃ½ láº¡i 1001 files

### 3.2 Giáº£i phÃ¡p: Incremental Sync

**File**: `src/ingestion/metadata.py`

```python
class MetadataManager:
    """Track file hashes and chunk IDs for incremental updates."""
    
    def calculate_file_hash(file_path):
        # MD5 hash cá»§a file content
        return hashlib.md5(file.read()).hexdigest()
```

**Metadata Registry** (`indexing_metadata.json`):
```json
{
    "luat_lao_dong.pdf": {
        "hash": "abc123def456",
        "chunk_ids": ["hash_0", "hash_1", "hash_2", ...],
        "last_indexed": "2026-01-27T10:00:00Z"
    },
    "nghi_dinh_145.pdf": {
        "hash": "xyz789abc012",
        "chunk_ids": ["hash_0", "hash_1"],
        "last_indexed": "2026-01-27T10:05:00Z"
    }
}
```

### 3.3 Sync Algorithm

```python
def sync_index():
    # 1. Scan current files vÃ  compute hashes
    current_hashes = {}
    for file in os.listdir(raw_data_path):
        current_hashes[file] = calculate_file_hash(file)
    
    # 2. Compare with metadata registry
    files_to_add = []     # New files
    files_to_update = []  # Changed files (hash mismatch)
    files_to_delete = []  # Removed files
    files_skipped = []    # Unchanged files
    
    # 3. Process changes
    for file in files_to_delete:
        vector_store.delete(old_chunk_ids)
        metadata.remove(file)
    
    for file in files_to_update:
        vector_store.delete(old_chunk_ids)
        new_chunks = load_and_split(file)
        vector_store.add(new_chunks)
        metadata.update(file, new_hash, new_chunk_ids)
    
    for file in files_to_add:
        new_chunks = load_and_split(file)
        vector_store.add(new_chunks)
        metadata.add(file, hash, chunk_ids)
```

### 3.4 Flow Diagram

```mermaid
sequenceDiagram
    participant CLI
    participant Indexer as VectorIndexer
    participant Meta as MetadataManager
    participant FAISS

    CLI->>Indexer: sync_index()
    Indexer->>Meta: Load File Registry
    Indexer->>Indexer: Scan raw/ & Compute Hashes
    
    loop For Each File
        alt Deleted
            Indexer->>FAISS: delete(old_chunk_ids)
            Indexer->>Meta: remove_entry()
        else Updated/Added
            Indexer->>Indexer: Load & Split
            Indexer->>FAISS: add_documents(ids=hash_idx)
            Indexer->>Meta: update_entry(hash, ids)
        else Skipped
            Indexer->>Indexer: Do nothing
        end
    end
    
    Indexer->>FAISS: save_local()
    Indexer->>Meta: save_metadata()
```

---

## 4. Chunk ID Strategy

### 4.1 Deterministic IDs
```python
chunk_id = f"{file_hash}_{chunk_index}"
# Example: "abc123def456_0", "abc123def456_1", ...
```

**Táº¡i sao dÃ¹ng file_hash?**
- Unique per file content
- Stable - same content = same hash
- Cho phÃ©p precise deletion khi file bá»‹ update

### 4.2 ID Mapping
```
FAISS internal ID: 0, 1, 2, 3, ...
         â†“ mapping
Custom chunk ID: "hash1_0", "hash1_1", "hash2_0", ...
```

---

## 5. Performance Considerations

### 5.1 Benchmarks (1500 vectors)
| Operation | Time |
|-----------|------|
| Load PDF (100 pages) | ~2-3s |
| Split into chunks | ~0.1s |
| Embed all chunks | ~5-10s |
| Create FAISS index | ~0.5s |
| Save to disk | ~0.2s |

### 5.2 IVF Training Requirements
```
Minimum vectors for IVF64: 64 (= nlist)
Recommended vectors: 39 * 64 = 2,496
```

**Fallback**: Náº¿u khÃ´ng Ä‘á»§ vectors, tá»± Ä‘á»™ng switch sang Flat index.

---

## 6. Code Walkthrough

### 6.1 Entry Point
```python
# ingest.py
from src.ingestion.indexer import VectorIndexer

if __name__ == "__main__":
    VectorIndexer.sync_index()
```

### 6.2 Streamlit UI Trigger
```python
# app.py
if st.button("ðŸ”„ Cáº­p nháº­t Index"):
    VectorIndexer.sync_index()
    st.cache_resource.clear()  # Clear cached retriever
    st.rerun()
```

---

## 7. Key Takeaways

> [!IMPORTANT]
> **Äiá»ƒm nháº¥n khi thuyáº¿t trÃ¬nh:**
> 1. **4-step pipeline**: Load â†’ Split â†’ Embed â†’ Index
> 2. **Incremental sync**: Chá»‰ xá»­ lÃ½ files thay Ä‘á»•i
> 3. **Metadata tracking**: JSON registry cho file hashes
> 4. **Deterministic IDs**: Cho phÃ©p precise updates/deletes

---

## TÃ i liá»‡u liÃªn quan
- [Text Chunking](./02_text_chunking.md)
- [Embedding Models](./03_embedding_models.md)
- [FAISS Vector Search](./04_faiss_vector_search.md)

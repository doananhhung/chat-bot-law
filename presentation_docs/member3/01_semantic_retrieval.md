# üîé Semantic Retrieval - T√¨m Ki·∫øm Ng·ªØ Nghƒ©a

## M·ª•c ti√™u h·ªçc t·∫≠p
Sau khi ƒë·ªçc t√†i li·ªáu n√†y, b·∫°n s·∫Ω hi·ªÉu:
- Semantic search kh√°c keyword search nh∆∞ th·∫ø n√†o
- C√°ch SemanticRetriever ho·∫°t ƒë·ªông
- Top-K retrieval v√† relevance scoring

---

## 1. Keyword Search vs Semantic Search

### 1.1 So s√°nh

| Aspect | Keyword Search | Semantic Search |
|--------|----------------|-----------------|
| **Matching** | Exact words | Meaning/concept |
| **Query** | "ngh·ªâ thai s·∫£n" | "ƒë∆∞·ª£c ngh·ªâ bao l√¢u khi sinh con?" |
| **Miss** | "ngh·ªâ ƒë·∫ª", "maternity leave" | ‚ùå |
| **Catch** | ‚úÖ All related concepts | ‚úÖ |

### 1.2 V√≠ d·ª• minh h·ªça

```
User Query: "ngh·ªâ ƒë·∫ª ƒë∆∞·ª£c m·∫•y th√°ng"

Keyword Search (TF-IDF, BM25):
‚ùå Kh√¥ng match "thai s·∫£n"
‚ùå Kh√¥ng match "sinh con"
‚ùå Miss relevant documents

Semantic Search (Embedding):
‚úÖ Match "ngh·ªâ thai s·∫£n"
‚úÖ Match "lao ƒë·ªông n·ªØ ƒë∆∞·ª£c ngh·ªâ khi sinh con"
‚úÖ Match "maternity leave"
‚Üí Understands MEANING, not just words
```

---

## 2. Semantic Search Process

### 2.1 High-Level Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   SEMANTIC RETRIEVAL                     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ   Query: "ngh·ªâ ƒë·∫ª m·∫•y th√°ng?"                          ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ
‚îÇ          ‚ñº                                              ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ
‚îÇ   ‚îÇ  Embedding  ‚îÇ  ‚Üê vietnamese-bi-encoder              ‚îÇ
‚îÇ   ‚îÇ   Model     ‚îÇ                                       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ
‚îÇ          ‚ñº                                              ‚îÇ
‚îÇ   Query Vector: [0.12, -0.34, ..., 0.78]               ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ
‚îÇ          ‚ñº                                              ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ
‚îÇ   ‚îÇ   FAISS     ‚îÇ  ‚Üê Similarity search                  ‚îÇ
‚îÇ   ‚îÇ   Index     ‚îÇ                                       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ
‚îÇ          ‚îÇ                                              ‚îÇ
‚îÇ          ‚ñº                                              ‚îÇ
‚îÇ   Top-K Documents (by similarity score)                 ‚îÇ
‚îÇ   ‚Ä¢ ƒêi·ªÅu 139: Ngh·ªâ thai s·∫£n... (score: 0.92)           ‚îÇ
‚îÇ   ‚Ä¢ ƒêi·ªÅu 140: Ch·∫ø ƒë·ªô khi mang thai... (score: 0.85)    ‚îÇ
‚îÇ   ‚Ä¢ ...                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Step by Step

1. **Embed Query**: Chuy·ªÉn c√¢u h·ªèi ‚Üí vector 768D
2. **Search FAISS**: T√¨m K vectors g·∫ßn nh·∫•t
3. **Map to Documents**: Convert vector IDs ‚Üí Document objects
4. **Return Results**: Tr·∫£ v·ªÅ top-K documents v·ªõi metadata

---

## 3. SemanticRetriever Class

### 3.1 Initialization

```python
# src/rag_engine/retriever.py

class SemanticRetriever:
    def __init__(self):
        # 1. Load embedding model
        self.embeddings = HuggingFaceEmbeddings(
            model_name=AppConfig.EMBEDDING_MODEL_NAME,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # 2. Load FAISS vector store
        self.vector_store = self._load_vector_store()
        
        # 3. Configure search parameters (IVF nprobe)
        self._configure_search_params()
```

### 3.2 Load Vector Store

```python
def _load_vector_store(self):
    try:
        return FAISS.load_local(
            AppConfig.VECTOR_DB_PATH,
            self.embeddings,
            allow_dangerous_deserialization=True
            # Safe because we created the index ourselves
        )
    except Exception as e:
        raise RuntimeError("Vector Store not found. Please run ingestion first.")
```

### 3.3 Get Relevant Documents

```python
def get_relevant_docs(self, query: str, k: int = 10) -> List[Document]:
    """
    Retrieve top k relevant documents for the query.
    """
    if not self.vector_store:
        return []

    logger.info(f"Searching for: '{query}'")
    docs = self.vector_store.similarity_search(query, k=k)
    logger.info(f"Found {len(docs)} relevant documents.")
    return docs
```

---

## 4. Top-K Retrieval

### 4.1 T·∫°i sao Top-K?

| V·∫•n ƒë·ªÅ | Gi·∫£i ph√°p |
|--------|-----------|
| LLM context limit | Kh√¥ng th·ªÉ ƒë∆∞a t·∫•t c·∫£ documents |
| Noise reduction | Ch·ªâ l·∫•y most relevant |
| Cost optimization | √çt tokens = r·∫ª h∆°n |

### 4.2 Ch·ªçn K nh∆∞ th·∫ø n√†o?

| K Value | Trade-off |
|---------|-----------|
| K = 3 | Risk missing relevant info |
| **K = 10** | ‚úÖ Balanced (d√πng trong d·ª± √°n) |
| K = 20 | Risk including noise |

### 4.3 Trong d·ª± √°n

```python
# Default K = 10
docs = self.retriever.get_relevant_docs(query, k=10)
```

---

## 5. Similarity Metrics

### 5.1 L2 Distance (Euclidean)

```
distance = sqrt(Œ£(a_i - b_i)¬≤)

Smaller distance = More similar
```

### 5.2 Cosine Similarity

```
similarity = (A ¬∑ B) / (||A|| √ó ||B||)

Range: [-1, 1]
1 = Identical direction
0 = Orthogonal
-1 = Opposite direction
```

### 5.3 Trong d·ª± √°n

```python
# FAISS uses L2 distance
index = faiss.index_factory(768, "IVF64,Flat", faiss.METRIC_L2)

# But vectors are normalized, so:
# L2¬≤ = 2 - 2√ócosine_similarity
# ‚Üí Effectively equivalent to cosine similarity
```

---

## 6. Search Mode Configuration

### 6.1 Available Modes

```python
def set_search_mode(self, mode: str):
    mode_config = {
        "quality": ivf_index.nlist,  # Search all 64 clusters
        "balanced": 8,               # Search 8/64 = 12.5%
        "speed": 2,                  # Search 2/64 = 3%
    }
    ivf_index.nprobe = mode_config[mode]
```

### 6.2 Mode Comparison

| Mode | nprobe | Recall | Speed | Use Case |
|------|--------|--------|-------|----------|
| quality | 64 | ~100% | Slowest | Critical accuracy |
| **balanced** | 8 | ~73% | Balanced | Daily use |
| speed | 2 | ~33% | Fastest | Quick queries |

### 6.3 Get Current Mode

```python
def get_current_search_mode(self) -> dict:
    return {
        "mode": mode,
        "nprobe": nprobe,
        "nlist": nlist,
        "is_ivf": True,
        "search_scope_pct": (nprobe / nlist) * 100
    }
```

---

## 7. Document Output Format

### 7.1 Document Object

```python
Document(
    page_content="ƒêi·ªÅu 139. Ngh·ªâ thai s·∫£n\n1. Lao ƒë·ªông n·ªØ ƒë∆∞·ª£c ngh·ªâ...",
    metadata={
        "source": "luat_lao_dong.pdf",
        "page": 45,
        "chunk_id": "abc123_0",
        "chunk_index": 0,
        "total_chunks": 150
    }
)
```

### 7.2 Usage in RAG

```python
# generator.py
docs = self.retriever.get_relevant_docs(query)

# Format for LLM
context_str = format_context(docs)
# ‚Üí "--- T√†i li·ªáu 1 ---\nNgu·ªìn: luat_lao_dong.pdf | Trang: 46\n..."

# Send to LLM
answer = llm.invoke({
    "context": context_str,
    "question": query
})
```

---

## 8. Performance Considerations

### 8.1 Latency Breakdown

| Step | Time |
|------|------|
| Embed query | ~80-100ms |
| FAISS search | ~10-30ms |
| Document mapping | ~5ms |
| **Total** | **~100-150ms** |

### 8.2 Caching

```python
# app.py uses Streamlit caching
@st.cache_resource
def get_retriever():
    return SemanticRetriever()

# Model loaded once, reused for all queries
```

### 8.3 Bottleneck Analysis

```
Cold start: ~17s  ‚Üê Embedding model loading (one-time)
Warm query: ~100ms ‚Üê Actual search (fast)
```

---

## 9. Edge Cases

### 9.1 No Results Found

```python
if not docs:
    return "T√¥i kh√¥ng t√¨m th·∫•y t√†i li·ªáu ph√°p l√Ω n√†o li√™n quan..."
```

### 9.2 Vector Store Not Ready

```python
try:
    retriever = SemanticRetriever()
except RuntimeError:
    st.error("Vector Store not found. Please run ingestion first.")
```

### 9.3 Low Similarity Scores

Trong d·ª± √°n hi·ªán t·∫°i, kh√¥ng filter theo similarity threshold.
C√≥ th·ªÉ c·∫£i ti·∫øn:

```python
# Potential improvement
docs = vector_store.similarity_search_with_score(query, k=10)
filtered = [doc for doc, score in docs if score < 0.5]  # L2 distance
```

---

## 10. Code Flow Summary

```python
# Complete retrieval flow

# 1. User enters query
query = "Thai s·∫£n ƒë∆∞·ª£c ngh·ªâ bao nhi√™u ng√†y?"

# 2. Get retriever (cached)
retriever = get_retriever()

# 3. Set search mode (optional)
retriever.set_search_mode("balanced")

# 4. Retrieve documents
docs = retriever.get_relevant_docs(query, k=10)

# 5. Documents ready for RAG
# Each doc has:
# - page_content: text chunk
# - metadata: source, page, chunk_id
```

---

## 11. Key Takeaways

> [!IMPORTANT]
> **ƒêi·ªÉm nh·∫•n khi thuy·∫øt tr√¨nh:**
> 1. **Semantic search**: Hi·ªÉu √Ω nghƒ©a, kh√¥ng ch·ªâ keyword
> 2. **Top-K = 10**: Balanced recall v√† precision
> 3. **Search modes**: quality/balanced/speed - trade-off accuracy vs speed
> 4. **~100ms latency**: Fast enough for realtime chat

---

## T√†i li·ªáu li√™n quan
- [Intent Routing](./02_intent_routing.md)
- [Prompt Engineering](./03_prompt_engineering.md)
